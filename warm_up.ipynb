{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from datasets import load_dataset, Dataset, DatasetDict\n","from transformers import AutoModelForSeq2SeqLM,AutoModelForCausalLM, AutoTokenizer, GenerationConfig, TrainingArguments, Trainer\n","import torch\n","import time\n","import pandas as pd\n","import numpy as np"]},{"cell_type":"markdown","metadata":{},"source":["# Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","\n","data = pd.read_csv(\"/kaggle/input/llm1234/train.csv\")\n","train_df, test_df = train_test_split(data, test_size=0.2, random_state=42)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["torch.random.manual_seed(0) \n","model = AutoModelForCausalLM.from_pretrained( \n","    \"microsoft/Phi-3-mini-4k-instruct\",  \n","    device_map=\"cuda\",  \n","    torch_dtype=\"auto\",  \n","    trust_remote_code=True,  \n",") \n","\n","tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\") "]},{"cell_type":"markdown","metadata":{},"source":["# Metric"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["MAP@3: 0.8333333333333334\n"]}],"source":["def mapk(actual, predicted, k=3):\n","    \"\"\"\n","    Computes the mean average precision at k (MAP@k).\n","    \n","    Parameters:\n","    actual : list\n","        A list of correct labels, where each entry corresponds to the correct label for a single question.\n","    predicted : list\n","        A list of lists. Each inner list contains the predicted labels for a single question.\n","    k : int, optional (default=3)\n","        The maximum number of predicted elements to consider.\n","        \n","    Returns:\n","    score : float\n","        The MAP@k score for the predictions.\n","    \"\"\"\n","    def apk(actual, predicted, k=3):\n","        \"\"\"\n","        Computes the average precision at k (AP@k) for a single observation.\n","        \"\"\"\n","        if len(predicted) > k:\n","            predicted = predicted[:k]\n","        \n","        for i, p in enumerate(predicted):\n","            if p == actual:\n","                return 1.0 / (i + 1.0)\n","        \n","        return 0.0\n","    \n","    return sum(apk(a, p, k) for a, p in zip(actual, predicted)) / len(actual)\n","\n","# Example usage\n","actual = ['A', 'B', 'C']\n","predicted = [['A', 'B', 'C'], ['D', 'B', 'C'], ['C', 'A', 'B']]\n","score = mapk(actual, predicted, k=3)\n","print(f'MAP@3: {score}')"]},{"cell_type":"markdown","metadata":{},"source":["# Zero shot inferencing "]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["index = 0\n","\n","question = train_df.iloc[index]['prompt']\n","A = train_df.iloc[index]['A']\n","B = train_df.iloc[index]['B']\n","C = train_df.iloc[index]['C']\n","D = train_df.iloc[index]['D']\n","E = train_df.iloc[index]['E']\n","answer = train_df.iloc[index]['answer']\n","\n","\n","\n","\n","example = f\"\"\"\n","Choose the correct answer for this question. To answer this question, let's analyze each option step by step \n","\n","Question : What is the capital of france\n","A : Lyon\n","B : Paris\n","C : London\n","D : Toulouse\n","E : Madrid\n","\n","\"\"\"\n","\n","prompt = f\"\"\"\n","Choose the correct answer for this question. To answer this question, let's analyze each option step by step \n","\n","Question : {question}\n","A : {A}\n","B : {B}\n","C : {C}\n","D : {D}\n","E : {E}\n","\n","\"\"\"\n","\n","\n","\n","messages = [ \n","    {\"role\": \"system\", \"content\": \"You are a helpful AI assistant for question answering.\"}, \n","    {\"role\": \"user\", \"content\": f\"{example}\"}, \n","    {\"role\": \"assistant\", \"content\": \"B\"}, \n","    {\"role\": \"user\", \"content\": f\"{prompt}\"}, \n","] \n","\n","pipe = pipeline( \n","    \"text-generation\", \n","    model=model, \n","    tokenizer=tokenizer, \n",") \n","\n","generation_args = { \n","    \"max_new_tokens\": 500, \n","    \"return_full_text\": False, \n","    \"temperature\": 0.0, \n","    \"do_sample\": False, \n","} \n","\n","output = pipe(messages, **generation_args) \n","responce = output[0]['generated_text'] \n","\n","dash_line = '-'.join('' for x in range(100))\n","print(dash_line)\n","print(f'INPUT PROMPT:\\n{prompt}')\n","print(dash_line)\n","print(f'Correct Answer:\\n{answer}\\n')\n","print(dash_line)\n","print(f'MODEL Prediction - ZERO SHOT:\\n{responce}')"]},{"cell_type":"markdown","metadata":{},"source":["---------------------------------------------------------------------------------------------------\n","INPUT PROMPT:\n","\n","Choose the correct answer for this question. To answer this question, let's analyze each option step by step \n","\n","Question : What is a Hilbert space in quantum mechanics?\n","\n","A : A complex vector space where the state of a classical mechanical system is described by a vector |Ψ⟩.\n","\n","B : A physical space where the state of a classical mechanical system is described by a vector |Ψ⟩.\n","\n","C : A physical space where the state of a quantum mechanical system is described by a vector |Ψ⟩.\n","\n","D : A mathematical space where the state of a classical mechanical system is described by a vector |Ψ⟩.\n","\n","E : A complex vector space where the state of a quantum mechanical system is described by a vector |Ψ⟩.\n","\n","\n","---------------------------------------------------------------------------------------------------\n","Correct Answer:\n","\n","E\n","\n","---------------------------------------------------------------------------------------------------\n","MODEL Prediction - ZERO SHOT:\n","\n"," E"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Initialize lists to hold actual and predicted values\n","actual_labels = []\n","predicted_labels = []\n","\n","pipe = pipeline( \n","        \"text-generation\", \n","        model=model, \n","        tokenizer=tokenizer, \n","    ) \n","\n","generation_args = { \n","    \"max_new_tokens\": 500, \n","    \"return_full_text\": False, \n","    \"temperature\": 0.0, \n","    \"do_sample\": False, \n","} \n","\n","for index, row in test_df.iterrows():\n","    question = row['prompt']\n","    A = row['A']\n","    B = row['B']\n","    C = row['C']\n","    D = row['D']\n","    E = row['E']\n","    answer = row['answer']\n","    \n","    # Add the correct answer to the actual labels list\n","    actual_labels.append(answer)\n","    \n","    prompt = f\"\"\"\n","    Choose the correct answer for this question. To answer this question, let's analyze each option step by step \n","\n","    Question : {question}\n","    A : {A}\n","    B : {B}\n","    C : {C}\n","    D : {D}\n","    E : {E}\n","\n","    \"\"\"\n","    \n","    # Construct the prompt\n","    messages = [ \n","    {\"role\": \"system\", \"content\": \"You are a helpful AI assistant for question answering.\"}, \n","    {\"role\": \"user\", \"content\": f\"{example}\"}, \n","    {\"role\": \"assistant\", \"content\": \"B\"}, \n","    {\"role\": \"user\", \"content\": f\"{prompt}\"}, \n","    ] \n","\n","    output = pipe(messages, **generation_args) \n","    responce = output[0]['generated_text'] \n","    \n","    # Convert the output to a list and add it to the predicted labels list\n","    predicted_labels.append([responce.strip()])\n","    \n","# Calculate the MAP@k\n","k = 3  # You can change k if needed\n","mapk_score = mapk(actual_labels, predicted_labels, k)\n","print(f'MAP@{k} with the original model - Zero Shot: {mapk_score}')"]},{"cell_type":"markdown","metadata":{},"source":["MAP@3 with the original model - Zero Shot: 0.775\n"]},{"cell_type":"markdown","metadata":{},"source":["# Finetuning"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["device = \"cuda\"\n","\n","def tokenize_function(example):\n","    start_prompt = 'Choose the correct answer for this question. Provide your response (A, B, C, D, or E) of the correct answer as the output.'\n","    question = f\"\\n\\n{example['prompt']}\"\n","    A = f\"\\n\\n{example['A']}\"\n","    B = f\"\\n\\n{example['B']}\"\n","    C = f\"\\n\\n{example['C']}\"\n","    D = f\"\\n\\n{example['D']}\"\n","    E = f\"\\n\\n{example['E']}\"\n","\n","    end_prompt = '\\n\\nResponse: '\n","    prompt = start_prompt + question + A + B + C + D + E + end_prompt\n","    \n","    # Tokenize the prompt\n","    input_ids = tokenizer(prompt, padding=\"max_length\", truncation=True, return_tensors=\"pt\").input_ids\n","    # Tokenize the correct answer (no need for .tolist())\n","    labels = tokenizer(example[\"answer\"], padding=\"max_length\", truncation=True, return_tensors=\"pt\").input_ids\n","    \n","    # Convert tensors to lists and store them in the DataFrame\n","    example['input_ids'] = input_ids.squeeze().tolist()\n","    example['labels'] = labels.squeeze().tolist()\n","    \n","    return example\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import torch\n","from torch.utils.data import Dataset  # Fixed import statement\n","\n","class CustomDataset(Dataset):\n","    def __init__(self, df, device):\n","        # Move tensors to GPU\n","        self.x = torch.stack([torch.tensor(vector) for vector in df[\"input_ids\"].values]).to(device)\n","        self.y = torch.stack([torch.tensor(vector) for vector in df[\"labels\"].values]).to(device)\n","\n","    def __len__(self):\n","        return len(self.x)\n","\n","    def __getitem__(self, idx):\n","        return {\"input_ids\": self.x[idx], \"labels\": self.y[idx]}\n","\n","# Apply the function to your DataFrame and reset index\n","train_df = train_df.reset_index(drop=True)\n","test_df = test_df.reset_index(drop=True)\n","\n","tokenized_dataset_train = train_df.apply(tokenize_function, axis=1)\n","tokenized_dataset_test = test_df.apply(tokenize_function, axis=1)\n","\n","# Convert DataFrames to Dataset objects\n","train_dataset = CustomDataset(tokenized_dataset_train,\"cpu\")\n","test_dataset = CustomDataset(tokenized_dataset_test,\"cpu\")\n","\n","# Create a DatasetDict object\n","dataset_dict = DatasetDict({\n","    'train': train_dataset,\n","    'test': test_dataset\n","})\n","\n","# Verify the structure\n","print(dataset_dict)\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["#### PEFT : Lora finetuning"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["# ! pip install peft"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from peft import LoraConfig, get_peft_model, TaskType\n","\n","lora_config = LoraConfig(\n","    r=4, # Rank\n","    lora_alpha=4,\n","    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"mlp.gate_up_proj\", \"mlp.down_proj\"],\n","    lora_dropout=0.05,\n","    bias=\"none\",\n","    task_type=TaskType.CAUSAL_LM\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Add LoRA adapter layers/parameters to the original LLM to be trained.\n","peft_model = get_peft_model(model, \n","                            lora_config)\n","# Move the model to CUDA\n","peft_model.to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["output_dir = f'./peft-question-answering-training-{str(int(time.time()))}'\n","\n","peft_training_args = TrainingArguments(\n","    output_dir=output_dir,\n","    # Replace auto_find_batch_size with a specific batch size\n","    per_device_train_batch_size=8,  # Adjust this value as needed\n","    learning_rate=1e-3,  # Higher learning rate than full fine-tuning.\n","    num_train_epochs=10,\n","    logging_steps=10,\n","    max_steps=10,\n","    save_safetensors=False  # Ensure safetensors is not used\n",")\n","    \n","peft_trainer = Trainer(\n","    model=peft_model,\n","    args=peft_training_args,\n","    train_dataset=dataset_dict[\"train\"],\n","\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["peft_trainer.train()\n","\n","peft_model_path=\"./peft-question-answering-checkpoint-local\"\n","\n","peft_trainer.model.save_pretrained(peft_model_path)\n","tokenizer.save_pretrained(peft_model_path)"]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"databundleVersionId":6169864,"sourceId":54662,"sourceType":"competition"},{"datasetId":3705022,"sourceId":6422564,"sourceType":"datasetVersion"}],"dockerImageVersionId":30746,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.9"}},"nbformat":4,"nbformat_minor":4}
